\section{Results \& Discussion}

Because this is a graph of root mean squared errors, lower RMSE scores are better. For nearly every response item, the technique with the lowest error was the baseline ZeroR technique in which the response with the most prevalence in the training data was assumed to be true for every validation prediction.  This generally works well, because participants in our dataset are not varying their answer much leaving a clear majority response which is often correct when we predict on new data. Many of the methods being used are having a hard time beating this simple baseline, partly because they are not leveraging this previous information as much as they possibly should be.

If we think of depression’s effects on mood as something that happens on a long term scale where its effects are slowly ebb and flow on a weekly or monthly scale, then we might expect that each participant is at a different point in this ebb and flow cycle and the effects of depression are either waxing or waning.  When we train a model using not just one participant’s data, but a few participant’s data and predict on a new person who was never represented in the original data, we may see that the ZeroR baseline is now performing much worse.  However, the big question is how much worse the learning algorithms do as well given that they also will not have seen any data from the participant.  There is a possibility that it will learn something about depression that is universal across participants and can predict on new participants as well, but I have doubts as to how prevalent this signal will be.

At the conclusion of this study, I will know whether the general approach that those in the field of mood monitoring have been taking to build models of depression that can be applied to new users is feasible and provide a set of general recommendations for future research that might help in this field.  This research provides a description of what to expect in terms of what these algorithms get right (true positives, true negatives) and what they get wrong (false positives, false negatives) and in what context they occur.  These findings, coupled with theory from behavior change and cognitive behavioral therapy, can help developers think about what an acceptable accuracy for such a system is.  How bad is the impact of these incorrect predictions on the user and will that affect the user’s trust in the system.

We will know how passive data collection systems coupled with experience sampling systems can help us to understand those living with depression and the difficulties associated with collecting that information in context in the wild as well as collecting that information through the qualitative interviewing part of our study. We will speak on the value of including open-ended questions as well as weekly interviews with users in determining the strengths and weaknesses of this approach.  Particularly, the strength of matching up predictions with user stories from interviews which helped us determine flaws in the user interface to elicit appropriate responses and flaws in the modeling approach which overfit on previous training data such that it could not be applied to new settings.

This study will open the conversation to not just focus on mental health monitoring as simply a process of modeling smartphone sensor and user interaction data as it relates to some response to questions like “How are you feeling today?”, but also an opportunity to help those living with depression achieve the kind of self-reflection that is generally a part of cognitive behavioral therapy and is almost a necessary prerequisite for providing informative feedback to a learning system.